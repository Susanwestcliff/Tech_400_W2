{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JqjT-zyuKylw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom stopwords\n",
        "STOPWORDS = {'the', 'is', 'in', 'it', 'and', 'to', 'a', 'of', 'for', 'on', 'with', 'as', 'by', 'at', 'an'}\n",
        "\n",
        "# Converting text to lowercase\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Removing URLs\n",
        "def remove_urls(text):\n",
        "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "# Removing special characters\n",
        "def remove_special_characters(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "# Removing digits\n",
        "def remove_digits(text):\n",
        "    return re.sub(r'\\d+', '', text)"
      ],
      "metadata": {
        "id": "Em6cPYs_WT3N"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "def tokenize_text(text):\n",
        "    return text.split()\n",
        "\n",
        "# Stopword removal\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in STOPWORDS]\n",
        "\n",
        "# Lemmatization\n",
        "def lemmatize_word(word):\n",
        "    if word.endswith('ing') or word.endswith('ed'):\n",
        "        word = word[:-3]\n",
        "    elif word.endswith('s'):\n",
        "        word = word[:-1]\n",
        "    return word\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatize_word(word) for word in tokens]"
      ],
      "metadata": {
        "id": "qgisZyCdc-Uy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full text preprocessing pipeline\n",
        "def process_text(text):\n",
        "    text = to_lowercase(text)\n",
        "    text = remove_urls(text)\n",
        "    text = remove_special_characters(text)\n",
        "    text = remove_digits(text)\n",
        "    tokens = tokenize_text(text)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    tokens = lemmatize_tokens(tokens)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "7i3Ta4hlc_Hl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IRSystem:\n",
        "    def __init__(self):\n",
        "        self.vocab = {}\n",
        "        self.docs = []\n",
        "        self.index = defaultdict(set)\n",
        "        self.doc_counter = 0\n",
        "        self.doc_map = {}\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        return process_text(text)\n",
        "\n",
        "    def add_doc(self, text, filename):\n",
        "        words = self.clean_text(text)\n",
        "        self.docs.append(text)\n",
        "        self.doc_map[self.doc_counter] = filename\n",
        "        for word in words:\n",
        "            if word not in self.vocab:\n",
        "                self.vocab[word] = len(self.vocab)\n",
        "            self.index[self.vocab[word]].add(self.doc_counter)\n",
        "        self.doc_counter += 1\n",
        "\n",
        "    def add_docs_from_folder(self, folder):\n",
        "        for file in os.listdir(folder):\n",
        "            if file.endswith('.txt'):\n",
        "                with open(os.path.join(folder, file), 'r', encoding='utf-8') as f:\n",
        "                    self.add_doc(f.read(), file)\n",
        "\n",
        "    def search(self, query):\n",
        "        words = self.clean_text(query)\n",
        "        result = None\n",
        "        for word in words:\n",
        "            word_id = self.vocab.get(word, -1)\n",
        "            if word_id == -1:\n",
        "                return set()\n",
        "            doc_ids = self.index[word_id]\n",
        "            result = doc_ids if result is None else result & doc_ids\n",
        "        return result\n",
        "\n",
        "    def get_filenames(self, doc_ids):\n",
        "        return [self.doc_map[doc_id] for doc_id in doc_ids]\n"
      ],
      "metadata": {
        "id": "izEZLAJbWT5p"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to read the dataset and process each line\n",
        "def main():\n",
        "    final_tokens = []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ir = IRSystem()\n",
        "    ir.add_docs_from_folder('/content/books')\n",
        "\n",
        "    input1 = \"Causes: bacteria deep wound infection susceptibility debility\"\n",
        "    input2 = \"The classification made is British matron\"\n",
        "\n",
        "    result1 = ir.search(input1)\n",
        "    result2 = ir.search(input2)\n",
        "\n",
        "    print(f\"'{input1}' is in document:\", ir.get_filenames(result1))\n",
        "    print('\\n')\n",
        "    print(f\"'{input2}' is in document:\", ir.get_filenames(result2))\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "id": "0pzdKdHxLJ5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9be269c3-dfea-451c-caa4-94a7a6e0449b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Causes: bacteria deep wound infection susceptibility debility' is in document: ['book4.txt']\n",
            "\n",
            "\n",
            "'The classification made is British matron' is in document: ['book1.txt', 'book3.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cYlz1sP5LJ8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gmxgP3v4LJ-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fe-aSWamLKA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vh2wt1__LKDV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}